# -*- coding: utf-8 -*-
"""HYPERparameters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11gSSibNMvlqOeaSJ7Q6sHt5XMGd3VdM2
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
file_path = 'emails.csv'
emails_df = pd.read_csv(file_path)

# Drop irrelevant columns and separate features and target
X = emails_df.drop(columns=['Email No.', 'Prediction'])
y = emails_df['Prediction']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X





# Define the hyperparameter grid for Grid Search
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest model
rf = RandomForestClassifier(random_state=42)

# Perform Grid Search
grid_search_rf = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=2, n_jobs=-1, verbose=1)
grid_search_rf.fit(X_train, y_train)

# Best parameters and best score from Grid Search
print("Best Parameters (Grid Search):", grid_search_rf.best_params_)
print("Best Score (Grid Search):", grid_search_rf.best_score_)

print(y_train.isnull().sum())

X_train = X_train[~y_train.isnull()]
y_train = y_train[~y_train.isnull()]

y_train.fillna(y_train.mode()[0], inplace=True)  # Replace with the most frequent value

grid_search_rf.fit(X_train, y_train)

# Define the hyperparameter space for Random Search
rf_param_dist = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform Randomized Search
random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=rf_param_dist, n_iter=10, cv=2, random_state=42, n_jobs=-1, verbose=1)
random_search_rf.fit(X_train, y_train)

# Best parameters and best score from Random Search
print("Best Parameters (Random Search):", random_search_rf.best_params_)
print("Best Score (Random Search):", random_search_rf.best_score_)

# Evaluate Grid Search best model
grid_best_rf = grid_search_rf.best_estimator_
y_pred_grid = grid_best_rf.predict(X_test)
print("Grid Search Model Accuracy:", accuracy_score(y_test, y_pred_grid))
print("Classification Report (Grid Search):\n", classification_report(y_test, y_pred_grid))

# Evaluate Random Search best model
random_best_rf = random_search_rf.best_estimator_
y_pred_random = random_best_rf.predict(X_test)
print("Random Search Model Accuracy:", accuracy_score(y_test, y_pred_random))
print("Classification Report (Random Search):\n", classification_report(y_test, y_pred_random))

# Convert Grid Search results to a DataFrame for visualization
results_grid = pd.DataFrame(grid_search_rf.cv_results_)

# Use pivot_table with an aggregation function to handle duplicate entries
pivot_table = results_grid.pivot_table(index="param_n_estimators", columns="param_max_depth", values="mean_test_score", aggfunc="mean")

# Plot heatmap for Grid Search results
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(pivot_table, annot=True, cmap="YlGnBu", cbar_kws={'label': 'Mean Test Score'})
plt.title("Grid Search Hyperparameter Tuning Results")
plt.xlabel("Max Depth")
plt.ylabel("Number of Estimators")
plt.show()

# Sort results by mean test score and get the top 10
results_random = pd.DataFrame(random_search_rf.cv_results_)
top_results = results_random.nlargest(10, "mean_test_score")

# Bar plot for Random Search Results
plt.figure(figsize=(10, 6))
sns.barplot(x="mean_test_score", y="param_n_estimators", hue="param_max_depth", data=top_results)
plt.title("Top 10 Random Search Results for Random Forest")
plt.xlabel("Mean Test Score")
plt.ylabel("Number of Estimators")
plt.legend(title="Max Depth")
plt.show()

accuracy_scores = {
    'RF Grid Search': accuracy_score(y_test, y_pred_grid),
    'RF Random Search': accuracy_score(y_test, y_pred_random)
}

# Convert to DataFrame for plotting
accuracy_df = pd.DataFrame(list(accuracy_scores.items()), columns=["Method", "Accuracy"])

# Plot accuracy comparison
plt.figure(figsize=(8, 5))
sns.barplot(x="Method", y="Accuracy", data=accuracy_df)
plt.title("Accuracy Comparison of Tuned Models")
plt.ylabel("Accuracy")
plt.show()

